{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pandas.core.frame import DataFrame\n",
    "from serpapi import GoogleSearch\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests import get\n",
    "import re \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from pprint import pprint\n",
    "from requests import session\n",
    "import textrazor\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "textrazor.api_key = \"cbf491222196a84d4bbcf85f575a75ea323c329bb97a2bb280404dac\"\n",
    "\n",
    "app_secret = 'e478f284e8aa736bc21fd8691ae7d08f14680d2e6a1fac7a8d6ad1f51e1b358f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "params = {\n",
    "  \"q\": \"Apple\",\n",
    "  \"tbm\": \"isch\",\n",
    "  \"ijn\": \"0\",\n",
    "  \"api_key\": app_secret\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textrazor\n",
    "\n",
    "textrazor.api_key = \"cbf491222196a84d4bbcf85f575a75ea323c329bb97a2bb280404dac\"\n",
    "\n",
    "client = textrazor.TextRazor(extractors=[\"entities\", \"words\"])\n",
    "client.set_cleanup_mode(cleanup_mode='cleanHTML')\n",
    "client.set_cleanup_return_cleaned(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.analyze_url('https://www.textrazor.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.399\n",
      "0\n",
      "0.2701\n",
      "0.4322\n",
      "0.3914\n",
      "0.2503\n",
      "0.2814\n",
      "0.2558\n",
      "0.1333\n",
      "0.288\n",
      "0.4295\n",
      "0.2389\n",
      "0.2512\n",
      "0.2471\n",
      "0.2471\n",
      "0.3735\n",
      "0.3344\n",
      "0.3553\n",
      "0.1322\n",
      "0.1268\n",
      "0.799\n",
      "0.3883\n",
      "0.3883\n",
      "0.4588\n",
      "0.3499\n",
      "0.2919\n",
      "0.3883\n",
      "0.3499\n",
      "0.3417\n",
      "0.1065\n",
      "0.2697\n",
      "0.2855\n",
      "0.1306\n",
      "0.2867\n",
      "0.3338\n",
      "0.2815\n",
      "0.1663\n",
      "0.4608\n",
      "0.4136\n",
      "0.1264\n",
      "0.1739\n",
      "0.6013\n",
      "0.1322\n",
      "0.3553\n",
      "0.2471\n",
      "0.3352\n",
      "0.3228\n",
      "0.2471\n",
      "0.1433\n",
      "0.3447\n",
      "0.3352\n",
      "0.3069\n",
      "0.4459\n",
      "0.2446\n",
      "0.3529\n",
      "0.3142\n",
      "0.1691\n",
      "0.4311\n",
      "0.3734\n",
      "0.3553\n",
      "0.3883\n",
      "0.4588\n",
      "0.3883\n",
      "0.2148\n",
      "0.1486\n",
      "0.3491\n",
      "0.3121\n",
      "0.4054\n",
      "0.4945\n",
      "0.2997\n",
      "0.4054\n",
      "0.6283\n",
      "0.6217\n",
      "0.1858\n",
      "0.6217\n",
      "0.4945\n",
      "0.2919\n",
      "0.3735\n",
      "0.6013\n",
      "0.6013\n",
      "0.6013\n",
      "0.6013\n",
      "0.6013\n",
      "0.6013\n",
      "0.6013\n",
      "0.4588\n",
      "0.4588\n",
      "0.1322\n",
      "0.1322\n",
      "0.1322\n",
      "0.1322\n",
      "0.1322\n",
      "0.3553\n",
      "0.3553\n",
      "0.3121\n",
      "0.3121\n",
      "0.3121\n",
      "0.4459\n",
      "0.799\n",
      "0.799\n",
      "0.799\n",
      "0.799\n",
      "0.799\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for e in response.entities():\n",
    "    print(e.json['relevanceScore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers \n",
    "## trian bert for multilabel classification \n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'eat': ['eats','eating','ate',]}\n",
    "related_terms = {}\n",
    "for key  in my_dict.keys():\n",
    "\n",
    "    params = {\n",
    "    \"q\": key,\n",
    "    \"tbm\": \"isch\",\n",
    "    \"ijn\": \"0\",\n",
    "    \"api_key\": app_secret\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    related_terms[key] = []\n",
    "    for item in results['suggested_searches']:\n",
    "        related_terms[key].append(item['name'])\n",
    "related_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_KEYWORDS = 50\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def sort_coo(coo_matrix):\n",
    "    \"\"\"Sort a dict with highest score\"\"\"\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature, score\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    print('results')\n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "def get_keywords(vectorizer, feature_names, doc):\n",
    "    \"\"\"Return top k keywords from a doc using TF-IDF method\"\"\"\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector = vectorizer.transform([doc])\n",
    "    \n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    #extract only TOP_K_KEYWORDS\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,TOP_K_KEYWORDS)\n",
    "    return keywords\n",
    "    print(keywords)\n",
    "    print(keywords.values)\n",
    "    print(tuple(keywords.keys,keywords.values))\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy\n",
    "\n",
    "# English pipelines include a rule-based lemmatizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "print(lemmatizer.mode)  # 'rule'\n",
    "\n",
    "doc = nlp(\"make, maker, making , remake\")\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple is looking at buying U.K. startup for $1 billion which is located 4 km away. Please bring 1 gallon of water and 2 gram of sugar\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed/processed.csv')\n",
    "## df has entity , url \n",
    "## count the number of url for each entity\n",
    "df.groupby('entity').count().sort_values(by='url',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "result = nlp(\"Apple is looking at buying U.K. startup for $1 billion which is located 4 km away. Please bring 1 gallon of water and 2 gram of sugar\")\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion which is located 4 km away. Please bring 1 gallon of water and 2 gram of sugar\"\n",
    "numeric_parirs = []\n",
    "previous_token = \"\"\n",
    "sentences = sent_tokenize(text)\n",
    "#numeric_df = pd.DataFrame(columns=['Value','Unit', 'Key','Doc','Sentence'])\n",
    "\n",
    "for sent in sentences:\n",
    "   result = nlp(sent)\n",
    "\n",
    "   for token_idx in range(len(result)):\n",
    "      token = result[token_idx]\n",
    "      if token.tag_ == \"CD\":\n",
    "         print(\"found NUM\")\n",
    "         if token_idx + 1 < len(result) :\n",
    "            next_token = result[token_idx + 1]\n",
    "            if next_token.tag_ == \"NN\" or next_token.tag_ == \"NNS\" or next_token.tag_ == \"SYM\":\n",
    "               print(\"found pair {} {}\".format(token.text, next_token.text))\n",
    "               numeric_parirs.append({'value':token.text, 'unit':next_token.text ,'sentence': sent })\n",
    "               key = token.text.lower() + \" \" + next_token.text.lower()\n",
    "               numeric_df.loc[len(numeric_df)] = {'Value':token.text, 'Unit':next_token.text ,'Key': key,'Sentence':sent,'Doc':1}\n",
    "            elif token_idx - 1 >= 0:\n",
    "                     prev_token = result[token_idx + 1]\n",
    "                     if prev_token.tag_ == \"NN\" or prev_token.tag_ == \"NNS\" or prev_token.tag_ == \"SYM\":\n",
    "                        print(\"found pair {} {}\".format(token.text, prev_token.text))\n",
    "                        key = token.text.lower() + \" \" + next_token.text.lower()\n",
    "                        numeric_parirs.append({'value':token.text, 'unit':prev_token.text, 'sentence': sent })\n",
    "                        numeric_df.loc[len(numeric_df)] = {'Value':token.text, 'Unit':prev_token.text ,'Key': key,'Sentence':sent,'Doc':2}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = numeric_df[numeric_df.duplicated(['Key','Doc']) == False]\n",
    "mydf2 = mydf[mydf.duplicated(['Key'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf2[['Key','Sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.adducation.info/how-to-improve-your-knowledge/units-of-measurement/'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table', attrs={'class':'dataList footable-loaded footable tablet breakpoint'})\n",
    "\n",
    "table_rows = table.find_all('tr')\n",
    "\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('th')\n",
    "    row = [i.text for i in td]\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table', attrs={'class':'dataList'})\n",
    "table_rows = table.find_all('tr')\n",
    "import pandas as pd \n",
    "mydf  = pd.DataFrame(columns=['unit'])\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td')\n",
    "    row = [i.text for i in td]\n",
    "    if len(row) > 0:\n",
    "        print('row {}'.format(row))\n",
    "        if len(row) > 3:\n",
    "            if \"or\" in row[2]:\n",
    "                units = row[2].split('or')\n",
    "                for item in units:\n",
    "                    print(item)\n",
    "                    mydf.loc[len(mydf)] = {'unit': item}\n",
    "            elif \"/\" in row[0]:\n",
    "                units = row[0].split('/')\n",
    "                for item in units:\n",
    "                    print(item)\n",
    "                    mydf.loc[len(mydf)] = {'unit': item}\n",
    "            else:\n",
    "                mydf.loc[len(mydf)] = {'unit': row[0]}\n",
    "                mydf.loc[len(mydf)] = {'unit': row[2]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.to_csv('units.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mydf = pd.read_csv('unit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences =  73\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "text = Passive\n",
    "doc = nlp(text)\n",
    "sents = list(doc.sents)\n",
    "print(\"Number of Sentences = \",len(sents))\n",
    "# for sent in doc.sents:\n",
    "#     for token in sent:\n",
    "#         print(token.dep_,token.tag_, end = \" \")\n",
    "#     print()\n",
    "passive_rule = [[{'DEP':'nsubjpass'},{'DEP':'aux','OP':'*'},{'DEP':'auxpass'},{'TAG':'VBN'}]]\n",
    "matcher.add('Passive',passive_rule)\n",
    "matches = matcher(doc)\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "text = Active\n",
    "doc = nlp(text)\n",
    "sents = list(doc.sents)\n",
    "print(\"Number of Sentences = \",len(sents))\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(token.dep_,token.tag_, end = \" \")\n",
    "    print()\n",
    "passive_rule = [[{'DEP':'nsubjpass'},{'DEP':'aux','OP':'*'},{'DEP':'auxpass'},{'TAG':'VBN'}]]\n",
    "matcher.add('Passive',passive_rule)\n",
    "matches = matcher(doc)\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Active = \"Harry ate six shrimp at dinner.\\\n",
    "Beautiful giraffes roam the savannah.\\\n",
    "Sue changed the flat tire.\\\n",
    "We are going to watch a movie tonight.\\\n",
    "I ran the obstacle course in record time.\\\n",
    "The crew paved the entire stretch of highway.\\\n",
    "Mom read the novel in one day.\\\n",
    "The critic wrote a scathing review.\\\n",
    "I will clean the house every Saturday.\\\n",
    "The staff is required to watch a safety video every year.\\\n",
    "She faxed her application for a new job.\\\n",
    "Tom painted the entire house.\\\n",
    "The teacher always answers the students’ questions.\\\n",
    "The choir really enjoys that piece.\\\n",
    "Who taught you to ski?\\\n",
    "The forest fire destroyed the whole suburb.\\\n",
    "The two kings are signing the treaty.\\\n",
    "The cleaning crew vacuums and dusts the office every night.\\\n",
    "Larry generously donated money to the homeless shelter.\\\n",
    "No one responded to my sales ad.\\\n",
    "The wedding planner is making all the reservations.\\\n",
    "Susan will bake two dozen cupcakes for the bake sale.\\\n",
    "The science class viewed the comet.\\\n",
    "Who ate the last cookie?\\\n",
    "Alex posted the video on Facebook.\\\n",
    "The director will give you instructions.\\\n",
    "Thousands of tourists view the Grand Canyon every year.\\\n",
    "The homeowners remodeled the house to help it sell.\\\n",
    "The team will celebrate their victory tomorrow.\\\n",
    "The saltwater eventually corroded the metal beams.\\\n",
    "The kangaroo carried her baby in her pouch.\\\n",
    "Some people raise sugar cane in Hawaii.\\\n",
    "He buys a camera.\\\n",
    "She drinks water.\\\n",
    "I know him.\\\n",
    "Water fills a tub.\\\n",
    "Women are not treated as equals.\\\n",
    "John has to study all afternoon.\\\n",
    "John is a good student.\\\n",
    "The dragon has scorched the metropolis with his fiery breath.\\\n",
    "After suitors invaded her house, Penelope had to think of ways to delay her remarriage.\\\n",
    "The Lao People’s Revolutionary Party set up a new system of drug control laws.\\\n",
    "Research points to heart disease as the leading cause of death in the United States.\\\n",
    "The surgeon positions the balloon in an area of blockage and inflates it.\\\n",
    "James writes the letters.\\\n",
    "James wrote the letters.\\\n",
    "James is writing the letters.\\\n",
    "James has written the letters.\\\n",
    "James is going to write the letters.\\\n",
    "James will write the letters.\\\n",
    "James was writing the letters.\\\n",
    "The scientists had found the cure, but it was too late.\\\n",
    "The scientists will have found a cure by then.\\\n",
    " I keep the butter in the fridge.\\\n",
    "John is keeping my house tidy.\\\n",
    "Mary kept her schedule meticulously.\\\n",
    "The theater was keeping a seat for you.\\\n",
    "I have kept all your old letters.\\\n",
    "He had kept up his training regimen for a month.\\\n",
    "Mark will keep the ficus.\\\n",
    "If you told me, I would keep your secret.\\\n",
    "I would have kept your bicycle here if you had left it with me.\\\n",
    "She wants to keep the book.\\\n",
    "Judy was happy to have kept the puppy.\\\n",
    "I have a feeling that you may be keeping a secret.\\\n",
    "Having kept the bird in a cage for so long, Jade wasn't sure it could survive in the wild.\\\n",
    "Guests might not play chess.\\\n",
    "He might meet Dewi.\\\n",
    "Dewi must not open the gate every morning.\\\n",
    "He must finish his duty in a week.\\\n",
    "I may not buy the computer.\\\n",
    "He may sell the house.\\\n",
    "May I buy the computer?\\\n",
    "Risky can not buy this car every time.\\\n",
    "She can sell the car every time.\\\n",
    "Can she play a violin?\"\n",
    "\n",
    "Passive = \"A camera is bought by him.\\\n",
    "Water is drunk by her.\\\n",
    "He is known to me.\\\n",
    "A tub is filled with water.\\\n",
    "Sugar is sold in kilograms.\\\n",
    "There is a considerable range of expertise demonstrated by the spam senders.\\\n",
    "It was determined by the committee that the report was inconclusive.\\\n",
    "We were invited by our neighbors to attend their party.\\\n",
    "Groups help participants realize that most of their problems and secrets are shared by others in the group.\\\n",
    "The proposed initiative will be bitterly opposed by abortion rights groups.\\\n",
    "Minor keys, modal movement, and arpeggios are shared by both musical traditions.\\\n",
    "In this way, the old religion was able to survive the onslaught of new ideas until the old gods were finally displaced by Christianity.\\\n",
    "First the apples are picked, then they are cleaned, and finally they’re packed and shipped to the market.\\\n",
    "New York is considered the most diverse city in the U.S.\\\n",
    "It is believed that Amelia Earhart’s plane crashed in Pacific Ocean.\\\n",
    "Hungarian is seen as one of the world’s most difficult languages to learn.\\\n",
    "Skin cancers are thought to be caused by excessive exposure to the sun.\\\n",
    "George Washington was elected president in 1788.\\\n",
    "Two people were killed in a drive-by shooting on Friday night.\\\n",
    "Ten children were injured when part of the school roof collapsed.\\\n",
    "I was hit by the dodgeball.\\\n",
    "The metropolis has been scorched by the dragon’s fiery breath.\\\n",
    "When her house was invaded, Penelope had to think of ways to delay her remarriage.\\\n",
    "A new system of drug control laws was set up.\\\n",
    "Heart disease is considered the leading cause of death in the United States.\\\n",
    "The balloon is positioned in an area of blockage and is inflated.\\\n",
    "The Exxon Company accepts that a few gallons might have been spilled.\\\n",
    "100 votes are required to pass the bill.\\\n",
    "Over 120 different contaminants have been dumped into the river.\\\n",
    "Baby Sophia was delivered at 3:30 a.m. yesterday.\\\n",
    "The letters are written by James.\\\n",
    "The letters were written by James.\\\n",
    "The letters are being written by James.\\\n",
    "The letters have been written by James.\\\n",
    "The letters are going to be written by James.\\\n",
    "The letters will be written by James.\\\n",
    "The letters were being written by James.\\\n",
    "The cure had been found, but it was too late.\\\n",
    "A cure will have been found by then.\\\n",
    "Mistakes were made.\\\n",
    "The butter is kept in the fridge.\\\n",
    "My house is being kept tidy.\\\n",
    "Mary's schedule was kept meticulously.\\\n",
    "A seat was being kept for you.\\\n",
    "All your old letters have been kept.\\\n",
    "His training regimen had been kept up for a month.\\\n",
    "The ficus will be kept.\\\n",
    "If you told me, your secret would be kept.\\\n",
    "Your bicycle would have been kept here if you had left it with me.\\\n",
    "The book wants to be kept.\\\n",
    "The puppy was happy to have been kept.\\\n",
    "I have a feeling that a secret may be being kept.\\\n",
    "The bird, having been kept in a cage for so long, might not survive in the wild.\\\n",
    "The car can be sold by her every time.\\\n",
    "Can a violin be played by her?\\\n",
    "The house may be sold by him.\\\n",
    "May the computer be bought by me?\\\n",
    "The computer may not be bought by me.\\\n",
    "His duty must be finished by him in a week.\\\n",
    "The gate must not be opened by Dewi every morning.\\\n",
    "Dewi might be met by him.\\\n",
    "Chess might not be played guests.\\\n",
    "At dinner, six shrimp were eaten by Harry.\\\n",
    "The savannah is roamed by beautiful giraffes.\\\n",
    "The flat tire was changed by Sue.\\\n",
    "A movie is going to be watched by us tonight.\\\n",
    "The obstacle course was run by me in record time.\\\n",
    "The entire stretch of highway was paved by the crew.\\\n",
    "The novel was read by Mom in one day.\\\n",
    "A scathing review was written by the critic.\\\n",
    "The house will be cleaned by me every Saturday.\\\n",
    "A safety video will be watched by the staff every year.\\\n",
    "The application for a new job was faxed by her.\\\n",
    "The entire house was painted by Tom.\\\n",
    "The students’ questions are always answered by the teacher.\\\n",
    "That piece is really enjoyed by the choir.\\\n",
    "By whom were you taught to ski?\\\n",
    "The whole suburb was destroyed by the forest fire.\\\n",
    "The treaty is being signed by the two kings.\\\n",
    "Every night the office is vacuumed and dusted by the cleaning crew.\\\n",
    "Money was generously donated to the homeless shelter by Larry.\\\n",
    "My sales ad was not responded to by anyone.\\\n",
    "All the reservations will be made by the wedding planner.\\\n",
    "For the bake sale, two dozen cookies will be baked by Susan.\\\n",
    "The comet was viewed by the science class.\\\n",
    "The video was posted on Facebook by Alex.\\\n",
    "Instructions will be given to you by the director.\\\n",
    "The Grand Canyon is viewed by thousands of tourists every year.\\\n",
    "The house was remodeled by the homeowners to help it sell.\\\n",
    "The victory will be celebrated by the team tomorrow.\\\n",
    "The metal beams were eventually corroded by the saltwater.\\\n",
    "The baby was carried by the kangaroo in her pouch.\\\n",
    "The last cookie was eaten by whom?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows = table.find_all('tr')\n",
    "\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('th')\n",
    "    row = [i.text for i in td]\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "mydf['unit'] = mydf['unit'].apply(lambda x: x.lower())\n",
    "difflib.get_close_matches('cm', mydf['unit'].values, cutoff=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difflib.get_close_matches('cm', mydf['unit'].values, cutoff=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = numeric_df[numeric_df.duplicated(['Key','Doc']) == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"4 km\"\n",
    "numeric_df[numeric_df.Key == key]['Sentence'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df[numeric_df.Key == '4 km']['Sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_df = numeric_df[numeric_df.duplicated('Key')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_df = numeric_df[numeric_df.duplicated('Key')]\n",
    "duplicated_df[['Key','Sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_parirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "numeric_parirs = []\n",
    "numeric_parirs.append({'value':4, 'unit':'km' ,'key':'4 km' })\n",
    "numeric_parirs.append({'value':1, 'unit':'m2','key':'4 km' })\n",
    "numeric_parirs.append({'value':2, 'unit':'days','key':'2 month' })\n",
    "numeric_parirs.append({'value':4, 'unit':'month', 'key':'1 km'})\n",
    "\n",
    "mydf = pd.DataFrame.from_dict(numeric_parirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.groupby(['key'])['key'].count().reset_index(name='Count').sort_values(['key','Count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mydf.groupby(['key'])['key'].count().reset_index(name='Count').sort_values(['key','Count'], ascending=False)\n",
    "df[df.Count > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "x = set([2])\n",
    "\n",
    "d['key'] = x\n",
    "myset = d['key']\n",
    "myset.add(5)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_parirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion which is located FOUR km away. Please bring 1 gallon of water and 2 gram of sugar\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,  token.pos_, token.tag_\n",
    "           , token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import lexeme\n",
    "\n",
    "t_razor_client = textrazor.TextRazor(extractors=[\"entities\", \"words\"])\n",
    "t_razor_client.set_cleanup_mode(cleanup_mode='cleanHTML')\n",
    "t_razor_client.set_cleanup_return_cleaned(True)\n",
    "        \n",
    "def search_keywords(input_text):\n",
    "    entity_df = pd.DataFrame(columns=['url','entity','entity_type','text'])\n",
    "    gsearch = GoogleSearch({\n",
    "        \"q\": input_text, \n",
    "        \"location\": \"Austin,Texas\",\n",
    "        \"num\" : \"5\",\n",
    "        \"api_key\": app_secret\n",
    "    })\n",
    "    result = gsearch.get_dict()\n",
    "\n",
    "    final_results= []\n",
    "\n",
    "    df = pd.DataFrame(columns=['link','title','text'])\n",
    "    count = 0\n",
    "    words_pos = {}\n",
    "    for item in result['organic_results']:\n",
    "        page_url = item['link']\n",
    "        title=item['title']\n",
    "        response = t_razor_client.analyze_url(item['link'])\n",
    "        response_obj = response.json\n",
    "        \n",
    "        for entity in response.entities():\n",
    "            if len(entity.freebase_types) > 0:\n",
    "                entity_df.loc[len(entity_df)] = {'url': page_url,'title':title, 'entity': str(entity.id).lower(), 'entity_type':str(entity.freebase_types[0]), 'text':response.cleaned_text }\n",
    "        entity_df['entity'] = entity_df['entity'].astype(str) \n",
    "        for sent in response_obj['response']['sentences']:\n",
    "            for word in sent['words']:\n",
    "                words_pos[word['token']] = word['partOfSpeech']\n",
    "        \n",
    "    return entity_df, words_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_razor_client = textrazor.TextRazor(extractors=[\"entities\", \"words\"])\n",
    "t_razor_client.set_cleanup_mode(cleanup_mode='cleanHTML')\n",
    "t_razor_client.set_cleanup_return_cleaned(True)\n",
    "gsearch = GoogleSearch({\n",
    "        \"q\": \"sprinkling water\", \n",
    "        \"location\": \"Austin,Texas\",\n",
    "        \"num\" : \"5\",\n",
    "        \"api_key\": app_secret\n",
    "    })\n",
    "result = gsearch.get_dict()\n",
    "\n",
    "for item in result['organic_results']:\n",
    "    response = t_razor_client.analyze_url(item['link'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = t_razor_client.analyze(\"i would like 4 pounds of rice and 2kg of sugar and a distance 2 m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_razor_client = textrazor.TextRazor(extractors=[\"entities\",\"words\"])\n",
    "\n",
    "file_name = \"best_yoga_mats_\"\n",
    "entity_df,words_pos = search_keywords('best yoga mats')\n",
    "url_df = entity_df['url'].unique()\n",
    "text_df =  entity_df['text'].unique()\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1,1),  smooth_idf=True, use_idf=True)\n",
    "vectorizer.fit_transform(list(text_df))\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "vectorizer_2 = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(2,2),  smooth_idf=True, use_idf=True)\n",
    "vectorizer_2.fit_transform(list(text_df))\n",
    "feature_names_2 = vectorizer_2.get_feature_names_out()\n",
    "\n",
    "vectorizer_3 = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(3,3),  smooth_idf=True, use_idf=True)\n",
    "vectorizer_3.fit_transform(list(text_df))\n",
    "feature_names_3 = vectorizer_3.get_feature_names_out()\n",
    "\n",
    "keywords_df = pd.DataFrame(columns=['text','keywords'])\n",
    "result = []\n",
    "corpora = text_df\n",
    "for doc in corpora:\n",
    "   \n",
    "    keywords = get_keywords(vectorizer, feature_names, doc)\n",
    "    keywords_df.loc[len(keywords_df)] = {'text':doc,'keywords':keywords}\n",
    "    keywords = get_keywords(vectorizer_2, feature_names_2, doc)\n",
    "    keywords_df.loc[len(keywords_df)] = {'text':doc,'keywords':keywords}\n",
    "    keywords = get_keywords(vectorizer_3, feature_names_3, doc)\n",
    "    keywords_df.loc[len(keywords_df)] = {'text':doc,'keywords':keywords}\n",
    "\n",
    "    \n",
    "print(keywords_df)\n",
    "\n",
    "\n",
    "unique_keys = set()\n",
    "for i in range(len(keywords_df)):\n",
    "     for item in keywords_df.iloc[i]['keywords']:\n",
    "           unique_keys.add(item)\n",
    "\n",
    "keywords_summary_df = pd.DataFrame(columns=['keyword','no_documents','average_weight','max','pos'])\n",
    "\n",
    "for u_key in unique_keys:\n",
    "    scores = []\n",
    "    cnt = 0\n",
    "    for i in range(len(keywords_df)):\n",
    "        for keyword in keywords_df.iloc[i]['keywords']:\n",
    "            if keyword == u_key:\n",
    "                cnt = cnt + 1\n",
    "                scores.append(keywords_df.iloc[i]['keywords'][u_key])\n",
    "                break\n",
    "    \n",
    "    pos = ''\n",
    "    if u_key in words_pos:\n",
    "        pos = words_pos[u_key]\n",
    "    keywords_summary_df.loc[len(keywords_summary_df)] = {'keyword': u_key, 'no_documents':cnt, 'average_weight':np.average(scores),'max': np.max(scores),'pos':pos}\n",
    "    keywords_summary_df.sort_values('average_weight',ascending=False)[:10]\n",
    "    #keywords_summary_df.sort_values('max',ascending=False).to_csv('{}keywords_including_stopwords.csv'.format(file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the benefit of yoghurt\"\n",
    "t_razor_client = textrazor.TextRazor(extractors=[\"entities\",\"words\"])\n",
    "result = t_razor_client.analyze(query)\n",
    "result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pattern.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pattern.en import lexeme\n",
    "\n",
    "try: \n",
    "    lexeme('dog')\n",
    "except: \n",
    "    pass\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "s_words = set(stopwords.words('english'))\n",
    "suffixes_prefixes = {}\n",
    "query = \"hell world country cat v dog meaw house\"\n",
    "for item in query.split():\n",
    "    if item not in s_words:\n",
    "       suffixes_prefixes[item] = lexeme(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "suf_pref_df = pd.DataFrame(ig=True).from_dict(suffixes_prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf_pref_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in query.split():\n",
    "    if item not in s_words:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_summary_df['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'thef d' in words_pos:\n",
    "    print(words_pos['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in keywords_summary_df.iterrows():\n",
    "    if row['keyword'] in words_pos:\n",
    "        row['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_summary_df['keyword'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_entities = []\n",
    "for entity in entity_df['entity'].unique():\n",
    "    if entity.lower() in keywords_summary_df['keyword'].unique() or entity.isdigit():\n",
    "        final_entities.append(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_keybert(data_df):\n",
    "        url_df = data_df.groupby(['url','text'])['url'].unique()\n",
    "        text_df =  data_df.groupby(['url','text'])['text'].unique()\n",
    "\n",
    "        kw_extractor = KeyBERT()\n",
    "        key_df = pd.DataFrame(columns=['url','text','keywords'])\n",
    "        for j in range(len(url_df)):\n",
    "            keywords = kw_extractor.extract_keywords(text_df[j], keyphrase_ngram_range=(1,3),stop_words='english',diversity=0.6,top_n=10,use_mmr=True,\n",
    "            )\n",
    "            key_df.loc[len(key_df)] = {'url': url_df[j],'text':text_df[j],'keywords':keywords}\n",
    "        return key_df\n",
    "def apply_keybert(data_df):\n",
    "        url_df = data_df.groupby(['url','text'])['url'].unique()\n",
    "        text_df =  data_df.groupby(['url','text'])['text'].unique()\n",
    "\n",
    "        kw_extractor = KeyBERT()\n",
    "        key_df = pd.DataFrame(columns=['url','text','keywords'])\n",
    "        for j in range(len(url_df)):\n",
    "            keywords = kw_extractor.extract_keywords(text_df[j], keyphrase_ngram_range=(1,3),stop_words='english',diversity=0.6,top_n=10,use_mmr=True,\n",
    "            )\n",
    "            key_df.loc[len(key_df)] = {'url': url_df[j],'text':text_df[j],'keywords':keywords}\n",
    "        return key_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_2 = TfidfVectorizer(analyzer='word',ngram_range=(2,2),  smooth_idf=True, use_idf=True)\n",
    "vectorizer_2.fit_transform(list(text_df))\n",
    "feature_names_2 = vectorizer_2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_3 = TfidfVectorizer(analyzer='word',ngram_range=(3,3),  smooth_idf=True, use_idf=True)\n",
    "vectorizer_3.fit_transform(list(text_df))\n",
    "feature_names_3 = vectorizer_3.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df['length'] = len(str(entity_df['entity']).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_df.iloc[0]['entity'].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df['length'] = entity_df.apply(lambda row: len(row['entity'].split()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "cnt = 0\n",
    "for u_key in unique_keys:\n",
    "    for i in range(len(keywords_df)):\n",
    "        for keyword in keywords_df.iloc[i]['keywords']:\n",
    "            if u_key == keyword:\n",
    "                cnt = cnt + 1\n",
    "                scores.append(keywords_df.iloc[i]['keywords'][u_key])\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keys = set()\n",
    "for i in range(len(keywords_df)):\n",
    "     for item in keywords_df.iloc[i]['keywords']:\n",
    "           unique_keys.add(item)\n",
    "\n",
    "keywords_summary_df = pd.DataFrame(columns=['keyword','no_documents','average_weight','max'])\n",
    "\n",
    "for u_key in unique_keys:\n",
    "    scores = []\n",
    "    cnt = 0\n",
    "    for i in range(len(keywords_df)):\n",
    "        for keyword in keywords_df.iloc[i]['keywords']:\n",
    "            if keyword == u_key:\n",
    "                cnt = cnt + 1\n",
    "                scores.append(keywords_df.iloc[i]['keywords'][u_key])\n",
    "                break\n",
    "    keywords_summary_df.loc[len(keywords_summary_df)] = {'keyword': u_key, 'no_documents':cnt, 'average_weight':np.average(scores),'max': np.max(scores)}\n",
    "    keywords_summary_df.sort_values('average_weight',ascending=False)[:10]\n",
    "    keywords_summary_df.sort_values('max',ascending=False).to_csv('keywords_including_stopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_summary_df.sort_values('max',ascending=False).to_csv('keywords_including_stopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_summary_df.sort_values('max',ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_summary_df.sort_values('max',ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "keywords_summary_df.sort_values('average_weight',ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_summary_df['keyword'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dict = {}\n",
    "keyword  no_documents avg_tf_idf\n",
    "\n",
    "keywords_weight_df = pd.DataFrame(columns=['keyword','no_documents','average_weight'])\n",
    "for i in range(len(keywords_df)):\n",
    "    for item_key in keywords_df.iloc[i]['keywords']:\n",
    "        \n",
    "keywords_weight_df.loc[len(keywords_weight_df)] = {'keyword':,'no_documents':,'average_weight':}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    keywords_weight_df = pd.DataFrame(columns=['keyword','no_documents','average_weight'])\n",
    "    keywords_weight_df.keyword == 'item'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dict = {}\n",
    "for idx, row in keywords_df.iterrows():\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df = pd.DataFrame(columns=['text','keywords'])\n",
    "result = []\n",
    "corpora = text_df\n",
    "for doc in corpora:\n",
    "   \n",
    "    keywords = get_keywords(vectorizer, feature_names, doc)\n",
    "    keywords_df.loc[len(keywords_df)] = {'text':doc,'keywords':keywords}\n",
    "    \n",
    "keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df.iloc[0]['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df.iloc[0]['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_grouped:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "#entity_df = search_keywords('Unlock Iphone')\n",
    "#kw_extractor = KeyBERT()\n",
    "#kw_extractor2 = KeyBERT('UBC-NLP/ARBERT')\n",
    "for j in range(len(df_grouped)):\n",
    "    keywords = kw_extractor.extract_keywords(df_grouped[j], keyphrase_ngram_range=(1,3),stop_words='english',nr_candidates=10,diversity=0.66,top_n=10,use_mmr=True,\n",
    "    )\n",
    "    print(\"Keywords of article\", str(j+1), \"\\n\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in entity_df.iterrows():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.groupby(['url','entity_type'])['entity_type'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mydf.groupby(['url','entity_type'])['entity_type'].count().reset_index(\n",
    "  name='Count').sort_values(['url','Count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = search_keywords(\"how to unlock iphone\")\n",
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textrazor\n",
    "import pandas as pd\n",
    "textrazor.api_key = \"cbf491222196a84d4bbcf85f575a75ea323c329bb97a2bb280404dac\"\n",
    "\n",
    "client = textrazor.TextRazor(extractors=[\"entities\", \"topics\"])\n",
    "client.set_cleanup_mode(cleanup_mode='cleanHTML')\n",
    "client.set_cleanup_return_cleaned(True)\n",
    "\n",
    "#response = client.analyze_url(\"http://www.bbc.co.uk/news/uk-politics-18640916\")\n",
    "\n",
    "entity_df = pd.DataFrame(columns=['entity','freebase_types'])\n",
    "for entity in response.entities():\n",
    "    entity_df.loc[len(entity_df)] = {'entity': entity.id}\n",
    "    #print(entity.id, entity.relevance_score, entity.confidence_score, entity.freebase_types)\n",
    "    if len(entity.freebase_types) > 0:\n",
    "        print(entity.id + \"_   \" + entity.freebase_types[0].replace('/','_'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a31c958b8b94db1955c9c6ffce5ff27dfb5d8ba7c0e60974445f323ccb21f2cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
